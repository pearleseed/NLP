# BÃ¡o cÃ¡o Lab 3: Word Embeddings

## 1. Má»¥c tiÃªu

TÃ¬m hiá»ƒu vÃ  á»©ng dá»¥ng **Word Embeddings** - ká»¹ thuáº­t biá»ƒu diá»…n tá»« dÆ°á»›i dáº¡ng dense vectors.

**CÃ¡c task thá»±c hiá»‡n:**
1. Táº£i vÃ  sá»­ dá»¥ng model pre-trained (GloVe)
2. NhÃºng vÄƒn báº£n (Document Embedding)
3. Huáº¥n luyá»‡n Word2Vec trÃªn dá»¯ liá»‡u nhá» (Gensim)
4. Huáº¥n luyá»‡n Word2Vec trÃªn dá»¯ liá»‡u lá»›n (Spark)
5. Trá»±c quan hÃ³a Embedding vá»›i t-SNE/PCA

---

## 2. Ná»n táº£ng LÃ½ thuyáº¿t

### 2.1. Tá»« Sparse sang Dense Representations

#### 2.1.1. Váº¥n Ä‘á» cá»§a One-hot vÃ  BoW
CÃ¡c phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng (One-hot, BoW, TF-IDF) táº¡o ra **sparse vectors**:

```
Vocabulary: [cat, dog, king, queen, man, woman] (6 tá»«)

One-hot encoding:
cat   = [1, 0, 0, 0, 0, 0]
dog   = [0, 1, 0, 0, 0, 0]
king  = [0, 0, 1, 0, 0, 0]
queen = [0, 0, 0, 1, 0, 0]
```

**NhÆ°á»£c Ä‘iá»ƒm:**
- **KhÃ´ng cÃ³ semantic similarity**: cos(king, queen) = 0, dÃ¹ cÃ³ quan há»‡ ngá»¯ nghÄ©a
- **Curse of dimensionality**: Vocabulary 100K tá»« â†’ vector 100K chiá»u
- **KhÃ´ng generalize**: Má»—i tá»« lÃ  má»™t chiá»u Ä‘á»™c láº­p

#### 2.1.2. Dense Word Embeddings
Word Embeddings biá»ƒu diá»…n tá»« báº±ng **dense vectors** vá»›i sá»‘ chiá»u nhá» (50-300):

```
king  = [0.50, 0.68, -0.59, 0.02, 0.60, ...]  (50-300 chiá»u)
queen = [0.45, 0.72, -0.55, 0.08, 0.58, ...]
```

**Æ¯u Ä‘iá»ƒm:**
- Capture Ä‘Æ°á»£c semantic similarity
- Sá»‘ chiá»u cá»‘ Ä‘á»‹nh, khÃ´ng phá»¥ thuá»™c vocabulary size
- CÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c quan há»‡ ngá»¯ nghÄ©a (analogy)

### 2.2. Distributional Hypothesis - Ná»n táº£ng lÃ½ thuyáº¿t

#### 2.2.1. PhÃ¡t biá»ƒu
> "You shall know a word by the company it keeps" - J.R. Firth (1957)
> 
> "Nhá»¯ng tá»« xuáº¥t hiá»‡n trong ngá»¯ cáº£nh tÆ°Æ¡ng tá»± cÃ³ xu hÆ°á»›ng mang Ã½ nghÄ©a tÆ°Æ¡ng Ä‘á»“ng."

#### 2.2.2. VÃ­ dá»¥ minh há»a
```
"The ___ sat on the mat."
"The ___ chased the mouse."
"I fed my ___ some milk."

â†’ CÃ¡c tá»« "cat", "dog", "kitten" cÃ³ thá»ƒ Ä‘iá»n vÃ o chá»— trá»‘ng
â†’ ChÃºng cÃ³ ngá»¯ cáº£nh tÆ°Æ¡ng tá»± â†’ cÃ³ nghÄ©a liÃªn quan
```

#### 2.2.3. Co-occurrence Matrix
Äáº¿m sá»‘ láº§n cÃ¡c tá»« xuáº¥t hiá»‡n cÃ¹ng nhau trong má»™t window:

```
Corpus: "I like deep learning. I like NLP. I enjoy flying."
Window size = 1 (chá»‰ xÃ©t tá»« liá»n ká»)

         I  like  deep  learning  NLP  enjoy  flying
I        0    3     0      0       0     1      0
like     3    0     1      1       1     0      0
deep     0    1     0      1       0     0      0
learning 0    1     1      0       0     0      0
NLP      0    1     0      0       0     0      0
enjoy    1    0     0      0       0     0      1
flying   0    0     0      0       0     1      0
```

### 2.3. Word2Vec - Chi tiáº¿t thuáº­t toÃ¡n

#### 2.3.1. Kiáº¿n trÃºc tá»•ng quan
Word2Vec (Mikolov et al., 2013) cÃ³ 2 kiáº¿n trÃºc:

```
CBOW (Continuous Bag of Words):
Context words â†’ [Average] â†’ Hidden Layer â†’ Target word
"The cat ___ on mat" â†’ predict "sat"

Skip-gram:
Target word â†’ Hidden Layer â†’ Context words
"sat" â†’ predict ["The", "cat", "on", "mat"]
```

#### 2.3.2. Skip-gram - CÃ´ng thá»©c chi tiáº¿t

**Má»¥c tiÃªu:** Maximize xÃ¡c suáº¥t cá»§a context words cho target word.

**Objective function:**
```
J(Î¸) = (1/T) Î£â‚œ Î£_{-câ‰¤jâ‰¤c, jâ‰ 0} log P(wâ‚œâ‚Šâ±¼ | wâ‚œ)
```

Trong Ä‘Ã³:
- `T`: Tá»•ng sá»‘ tá»« trong corpus
- `c`: Window size (context size)
- `wâ‚œ`: Target word táº¡i vá»‹ trÃ­ t
- `wâ‚œâ‚Šâ±¼`: Context word

**Softmax probability:**
```
P(wâ‚’ | wáµ¢) = exp(vâ‚’'áµ€ váµ¢) / Î£_{wâˆˆV} exp(váµ¥'áµ€ váµ¢)
```

Trong Ä‘Ã³:
- `váµ¢`: Input vector cá»§a word i (target)
- `vâ‚’'`: Output vector cá»§a word o (context)
- `V`: Vocabulary

**Váº¥n Ä‘á»:** Softmax tÃ­nh trÃªn toÃ n bá»™ vocabulary ráº¥t tá»‘n kÃ©m (|V| cÃ³ thá»ƒ > 100K)

#### 2.3.3. Negative Sampling - Giáº£i phÃ¡p tá»‘i Æ°u

Thay vÃ¬ tÃ­nh softmax trÃªn toÃ n bá»™ V, chá»‰ sample k negative examples:

**Objective vá»›i Negative Sampling:**
```
log Ïƒ(vâ‚’'áµ€ váµ¢) + Î£â‚– ğ”¼_{wâ‚–~Pâ‚™(w)} [log Ïƒ(-vâ‚–'áµ€ váµ¢)]
```

Trong Ä‘Ã³:
- `Ïƒ(x) = 1/(1 + eâ»Ë£)`: Sigmoid function
- `Pâ‚™(w)`: Noise distribution (thÆ°á»ng lÃ  unigram^0.75)
- `k`: Sá»‘ negative samples (thÆ°á»ng 5-20)

**Ã tÆ°á»Ÿng:**
- Positive sample: (target, context) thá»±c sá»± xuáº¥t hiá»‡n cÃ¹ng nhau â†’ maximize
- Negative samples: (target, random_word) khÃ´ng xuáº¥t hiá»‡n cÃ¹ng nhau â†’ minimize

#### 2.3.4. CBOW - Continuous Bag of Words

**Má»¥c tiÃªu:** Dá»± Ä‘oÃ¡n target word tá»« context words.

```
Input: Average cá»§a context word vectors
       h = (1/2c) Î£_{-câ‰¤jâ‰¤c, jâ‰ 0} vâ‚œâ‚Šâ±¼

Output: Softmax over vocabulary
       P(wâ‚œ | context) = softmax(Wâ‚’áµ€ h)
```

**So sÃ¡nh CBOW vs Skip-gram:**

| TiÃªu chÃ­ | CBOW | Skip-gram |
|----------|------|-----------|
| Tá»‘c Ä‘á»™ training | Nhanh hÆ¡n | Cháº­m hÆ¡n |
| Tá»« hiáº¿m | KÃ©m hÆ¡n | Tá»‘t hÆ¡n |
| Dataset nhá» | Tá»‘t hÆ¡n | KÃ©m hÆ¡n |
| Syntactic tasks | Tá»‘t hÆ¡n | TÆ°Æ¡ng Ä‘Æ°Æ¡ng |
| Semantic tasks | TÆ°Æ¡ng Ä‘Æ°Æ¡ng | Tá»‘t hÆ¡n |

### 2.4. GloVe - Global Vectors

#### 2.4.1. Ã tÆ°á»Ÿng chÃ­nh
GloVe (Pennington et al., 2014) káº¿t há»£p:
- **Matrix factorization** (nhÆ° LSA): Sá»­ dá»¥ng thá»‘ng kÃª toÃ n cá»¥c
- **Local context window** (nhÆ° Word2Vec): Há»c tá»« ngá»¯ cáº£nh cá»¥c bá»™

#### 2.4.2. Co-occurrence Probability Ratio

**Quan sÃ¡t quan trá»ng:**
```
XÃ©t cÃ¡c tá»«: ice, steam, solid, gas, water

P(solid | ice) / P(solid | steam) = large  (solid liÃªn quan ice)
P(gas | ice) / P(gas | steam) = small      (gas liÃªn quan steam)
P(water | ice) / P(water | steam) â‰ˆ 1      (water liÃªn quan cáº£ hai)
```

â†’ Ratio cá»§a co-occurrence probabilities encode thÃ´ng tin ngá»¯ nghÄ©a

#### 2.4.3. Objective Function

```
J = Î£áµ¢â±¼ f(Xáµ¢â±¼) (wáµ¢áµ€ wÌƒâ±¼ + báµ¢ + bÌƒâ±¼ - log Xáµ¢â±¼)Â²
```

Trong Ä‘Ã³:
- `Xáµ¢â±¼`: Co-occurrence count cá»§a word i vÃ  j
- `wáµ¢, wÌƒâ±¼`: Word vectors
- `báµ¢, bÌƒâ±¼`: Bias terms
- `f(x)`: Weighting function Ä‘á»ƒ giáº£m áº£nh hÆ°á»Ÿng cá»§a tá»« quÃ¡ phá»• biáº¿n

**Weighting function:**
```
f(x) = (x/xâ‚˜â‚â‚“)^Î±  if x < xâ‚˜â‚â‚“
     = 1           otherwise

(thÆ°á»ng Î± = 0.75, xâ‚˜â‚â‚“ = 100)
```

### 2.5. FastText - Subword Embeddings

#### 2.5.1. Váº¥n Ä‘á» OOV (Out-of-Vocabulary)
Word2Vec vÃ  GloVe khÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c tá»« má»›i khÃ´ng cÃ³ trong training data.

#### 2.5.2. Giáº£i phÃ¡p cá»§a FastText
Biá»ƒu diá»…n tá»« báº±ng tá»•ng cá»§a character n-grams:

```
word = "where", n = 3

Character n-grams: <wh, whe, her, ere, re>
(< vÃ  > lÃ  boundary markers)

v("where") = v(<wh) + v(whe) + v(her) + v(ere) + v(re>) + v(<where>)
```

**Æ¯u Ä‘iá»ƒm:**
- Xá»­ lÃ½ Ä‘Æ°á»£c OOV words
- Capture Ä‘Æ°á»£c morphology (tiá»n tá»‘, háº­u tá»‘)
- Tá»‘t cho ngÃ´n ngá»¯ cÃ³ nhiá»u biáº¿n thá»ƒ tá»« (tiáº¿ng Äá»©c, tiáº¿ng Thá»• NhÄ© Ká»³)

### 2.6. Word Analogy - Kiá»ƒm chá»©ng Embeddings

#### 2.6.1. Analogy Task
```
"king" - "man" + "woman" â‰ˆ "queen"

v(king) - v(man) + v(woman) â‰ˆ v(queen)
```

#### 2.6.2. CÃ¡c loáº¡i Analogy

| Loáº¡i | VÃ­ dá»¥ |
|------|-------|
| Gender | king:queen :: man:woman |
| Plural | cat:cats :: dog:dogs |
| Tense | walk:walked :: run:ran |
| Country-Capital | France:Paris :: Japan:Tokyo |
| Comparative | good:better :: bad:worse |

#### 2.6.3. Giáº£i thÃ­ch toÃ¡n há»c
Analogy hoáº¡t Ä‘á»™ng vÃ¬ word vectors encode cÃ¡c quan há»‡ nhÆ° **linear offsets**:

```
v(king) - v(queen) â‰ˆ v(man) - v(woman) â‰ˆ v(male) - v(female)

â†’ CÃ³ má»™t "gender direction" trong embedding space
```

### 2.7. Cosine Similarity cho Word Embeddings

#### 2.7.1. CÃ´ng thá»©c
```
similarity(A, B) = cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
                 = Î£áµ¢(Aáµ¢ Ã— Báµ¢) / (âˆšÎ£áµ¢(Aáµ¢Â²) Ã— âˆšÎ£áµ¢(Báµ¢Â²))
```

#### 2.7.2. Táº¡i sao dÃ¹ng Cosine?
- Word vectors Ä‘Ã£ Ä‘Æ°á»£c normalize vá» cÃ¹ng scale
- Cosine Ä‘o gÃ³c giá»¯a vectors, khÃ´ng phá»¥ thuá»™c magnitude
- GiÃ¡ trá»‹ trong [-1, 1], dá»… interpret

### 2.8. Document Embedding tá»« Word Embeddings

#### 2.8.1. Mean Pooling (Simple Average)
```
doc_vector = (1/n) Î£áµ¢ v(wordáµ¢)
```

**Æ¯u Ä‘iá»ƒm:** ÄÆ¡n giáº£n, nhanh
**NhÆ°á»£c Ä‘iá»ƒm:** Máº¥t thÃ´ng tin thá»© tá»±, tá»« quan trá»ng bá»‹ "pha loÃ£ng"

#### 2.8.2. Weighted Average (TF-IDF weighted)
```
doc_vector = Î£áµ¢ tfidf(wordáµ¢) Ã— v(wordáµ¢) / Î£áµ¢ tfidf(wordáµ¢)
```

#### 2.8.3. CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ¢ng cao
- **Doc2Vec (Paragraph Vectors)**: Há»c document vector cÃ¹ng vá»›i word vectors
- **Sentence-BERT**: DÃ¹ng Transformer Ä‘á»ƒ táº¡o sentence embeddings

### 2.9. Háº¡n cháº¿ cá»§a Static Word Embeddings

| Háº¡n cháº¿ | MÃ´ táº£ | VÃ­ dá»¥ |
|---------|-------|-------|
| Polysemy | Má»—i tá»« chá»‰ cÃ³ 1 vector | "bank" (ngÃ¢n hÃ ng) = "bank" (bá» sÃ´ng) |
| Context-independent | KhÃ´ng thay Ä‘á»•i theo ngá»¯ cáº£nh | "I love you" vs "Love is blind" |
| Bias | Há»c bias tá»« training data | "doctor" gáº§n "man", "nurse" gáº§n "woman" |

â†’ CÃ¡c háº¡n cháº¿ nÃ y dáº«n Ä‘áº¿n sá»± phÃ¡t triá»ƒn cá»§a **Contextualized Embeddings** (ELMo, BERT - Lab 6)

---

## 3. CÃ i Ä‘áº·t

### 3.1. Source Code
- `src/representations/word_embedder.py`: Lá»›p `WordEmbedder`
  - `get_vector(word)`: Láº¥y vector, tráº£ vá» vector 0 náº¿u OOV
  - `get_similarity(w1, w2)`: Cosine similarity
  - `get_most_similar(word, top_n)`: TÃ¬m tá»« Ä‘á»“ng nghÄ©a
  - `embed_document(doc)`: Mean pooling cÃ¡c word vectors

### 3.2. Model & Dataset
- **Pre-trained**: `glove-wiki-gigaword-50` (50D, ~65MB)
- **Toy corpus**: 6 cÃ¢u Ä‘Æ¡n giáº£n Ä‘á»ƒ demo huáº¥n luyá»‡n

---

## 4. Káº¿t quáº£

### 4.1. Task 1: Pre-trained Model (GloVe)

**Vector cá»§a 'king'** (5 pháº§n tá»­ Ä‘áº§u): `[0.50451, 0.68607, -0.59517, -0.022801, 0.60046]`
- KÃ­ch thÆ°á»›c vector: 50 chiá»u

| Cáº·p tá»« | Similarity | Giáº£i thÃ­ch |
|--------|------------|------------|
| king - queen | 0.7839 | Cao vÃ¬ cÃ¹ng trÆ°á»ng ngá»¯ nghÄ©a "hoÃ ng gia" |
| king - man | 0.5309 | Tháº¥p hÆ¡n, thá»ƒ hiá»‡n má»‘i quan há»‡ giá»›i tÃ­nh |

**10 tá»« tÆ°Æ¡ng Ä‘á»“ng nháº¥t vá»›i 'computer':**
| Tá»« | Similarity |
|----|------------|
| computers | 0.9165 |
| software | 0.8815 |
| technology | 0.8526 |
| electronic | 0.8126 |
| internet | 0.8060 |
| computing | 0.8026 |
| devices | 0.8016 |
| digital | 0.7992 |
| applications | 0.7913 |
| pc | 0.7883 |

**Nháº­n xÃ©t**: GloVe náº¯m báº¯t tá»‘t má»‘i quan há»‡ ngá»¯ nghÄ©a - cÃ¡c tá»« liÃªn quan cÃ´ng nghá»‡ cÃ³ similarity cao vá»›i "computer"

### 4.2. Task 2: Document Embedding
**CÃ¢u**: "The queen rules the country."
- **Vector** (5 pháº§n tá»­ Ä‘áº§u): `[0.02444, 0.37802, -0.63817, 0.01280, 0.05243]`
- **KÃ­ch thÆ°á»›c**: 50 chiá»u (mean pooling cá»§a cÃ¡c word vectors)

### 4.3. Task 3: So sÃ¡nh Model tá»± huáº¥n luyá»‡n vs Pre-trained

| Metric | Model tá»± huáº¥n luyá»‡n | GloVe Pre-trained |
|--------|---------------------|-------------------|
| Similarity 'king'-'queen' | 0.0560 | 0.7839 |
| Most similar to 'king' | cat, woman, the, prince, is | queen, prince, royal... |

**PhÃ¢n tÃ­ch:**
- Model tá»± huáº¥n luyá»‡n cho káº¿t quáº£ **ráº¥t kÃ©m** (similarity chá»‰ 0.056)
- NguyÃªn nhÃ¢n: Corpus chá»‰ cÃ³ 6 cÃ¢u Ä‘Æ¡n giáº£n, khÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ há»c má»‘i quan há»‡ ngá»¯ nghÄ©a
- **Káº¿t luáº­n**: Pre-trained models tiáº¿t kiá»‡m tÃ i nguyÃªn vÃ  cho káº¿t quáº£ tá»‘t hÆ¡n nhiá»u

### 4.4. Task 4: Spark MLlib
**5 tá»« Ä‘á»“ng nghÄ©a vá»›i 'data':**
| Tá»« | Similarity |
|----|------------|
| engine | 0.1237 |
| powerful | 0.0811 |
| spark | 0.0651 |
| quickly | 0.0441 |
| unified | 0.0420 |

**Nháº­n xÃ©t**: Káº¿t quáº£ kÃ©m do corpus demo quÃ¡ nhá» (chá»‰ 3 cÃ¢u), nhÆ°ng minh há»a Ä‘Æ°á»£c quy trÃ¬nh huáº¥n luyá»‡n phÃ¢n tÃ¡n vá»›i Spark

### 4.5. Task 5: Trá»±c quan hÃ³a (t-SNE)
- CÃ¡c tá»« cÃ¹ng trÆ°á»ng ngá»¯ nghÄ©a táº¡o thÃ nh cá»¥m riÃªng biá»‡t trÃªn biá»ƒu Ä‘á»“ 2D
- **Cá»¥m hoÃ ng gia**: king, queen, prince, princess
- **Cá»¥m quá»‘c gia**: country, nation, kingdom
- **Cá»¥m cÃ´ng nghá»‡**: computer, software, technology
- t-SNE giáº£m chiá»u tá»« 50D xuá»‘ng 2D Ä‘á»ƒ trá»±c quan hÃ³a, giá»¯ Ä‘Æ°á»£c cáº¥u trÃºc cá»¥m

---

## 5. Nháº­n xÃ©t

**Æ¯u Ä‘iá»ƒm Pre-trained Models:**
- Tiáº¿t kiá»‡m tÃ i nguyÃªn
- Táº­n dá»¥ng tri thá»©c tá»« corpus khá»•ng lá»“

**Háº¡n cháº¿:**
- OOV: KhÃ´ng xá»­ lÃ½ tá»« má»›i/hiáº¿m
- Static: KhÃ´ng phÃ¢n biá»‡t ngá»¯ cáº£nh (bank = ngÃ¢n hÃ ng = bá» sÃ´ng)

---

## 6. KhÃ³ khÄƒn & Giáº£i phÃ¡p

| Váº¥n Ä‘á» | Giáº£i phÃ¡p |
|--------|-----------|
| OOV | Tráº£ vá» vector 0, bá» qua khi embed |
| RAM | DÃ¹ng model nhá» (50D) hoáº·c Spark |
| t-SNE cháº­m | Chá»‰ visualize 20-30 tá»« Ä‘áº¡i diá»‡n |

---

## 7. TrÃ­ch dáº«n
- Gensim: https://radimrehurek.com/gensim/
- Scikit-learn: https://scikit-learn.org/
- Apache Spark: https://spark.apache.org/
- GloVe: glove-wiki-gigaword-50 via Gensim API
