{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lab 2: Count Vectorization (OOP approach)\n",
    "\n",
    "**Mục tiêu:** Triển khai kỹ thuật Count Vectorization (Bag-of-Words) theo hướng đối tượng.\n",
    "\n",
    "**Nội dung:**\n",
    "1. Test SimpleTokenizer và RegexTokenizer\n",
    "2. Test CountVectorizer trên toy corpus\n",
    "3. Test trên dataset UD English EWT\n",
    "4. So sánh với scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "from preprocessing.tokenizers import SimpleTokenizer, RegexTokenizer\n",
    "from representations.count_vectorizer import CountVectorizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test SimpleTokenizer và RegexTokenizer (Ví dụ cơ bản)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKENIZER COMPARISON\n",
      "============================================================\n",
      "\n",
      "Input: 'Hello World!'\n",
      "  SimpleTokenizer: ['Hello', 'World!']\n",
      "  RegexTokenizer:  ['hello', 'world']\n",
      "\n",
      "Input: 'I love NLP.'\n",
      "  SimpleTokenizer: ['I', 'love', 'NLP.']\n",
      "  RegexTokenizer:  ['i', 'love', 'nlp']\n",
      "\n",
      "Input: 'Natural Language Processing is fun!'\n",
      "  SimpleTokenizer: ['Natural', 'Language', 'Processing', 'is', 'fun!']\n",
      "  RegexTokenizer:  ['natural', 'language', 'processing', 'is', 'fun']\n",
      "\n",
      "Input: '   Multiple   spaces   here   '\n",
      "  SimpleTokenizer: ['Multiple', 'spaces', 'here']\n",
      "  RegexTokenizer:  ['multiple', 'spaces', 'here']\n",
      "\n",
      "Input: 'Numbers: 123, 456.78'\n",
      "  SimpleTokenizer: ['Numbers:', '123,', '456.78']\n",
      "  RegexTokenizer:  ['numbers', '123', '456', '78']\n"
     ]
    }
   ],
   "source": [
    "# Test cases\n",
    "test_sentences = [\n",
    "    \"Hello World!\",\n",
    "    \"I love NLP.\",\n",
    "    \"Natural Language Processing is fun!\",\n",
    "    \"   Multiple   spaces   here   \",\n",
    "    \"Numbers: 123, 456.78\"\n",
    "]\n",
    "\n",
    "simple_tokenizer = SimpleTokenizer()\n",
    "regex_tokenizer = RegexTokenizer()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZER COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nInput: '{sentence}'\")\n",
    "    print(f\"  SimpleTokenizer: {simple_tokenizer.tokenize(sentence)}\")\n",
    "    print(f\"  RegexTokenizer:  {regex_tokenizer.tokenize(sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nhận xét về Tokenizers:\n",
    "- **SimpleTokenizer**: Chỉ split theo whitespace, giữ nguyên dấu câu và case\n",
    "- **RegexTokenizer**: Dùng regex `\\b\\w+\\b`, lowercase, loại bỏ dấu câu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test CountVectorizer (Toy Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'a': 0, 'ai': 1, 'i': 2, 'is': 3, 'love': 4, 'nlp': 5, 'of': 6, 'programming': 7, 'subfield': 8}\n",
      "\n",
      "Vocabulary size: 9\n",
      "\n",
      "Document-Term Matrix:\n",
      "  Doc 0: [0, 0, 1, 0, 1, 1, 0, 0, 0]  <- 'I love NLP.'\n",
      "  Doc 1: [0, 0, 1, 0, 1, 0, 0, 1, 0]  <- 'I love programming.'\n",
      "  Doc 2: [1, 1, 0, 1, 0, 1, 1, 0, 1]  <- 'NLP is a subfield of AI.'\n"
     ]
    }
   ],
   "source": [
    "# Corpus từ đề bài\n",
    "corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"I love programming.\",\n",
    "    \"NLP is a subfield of AI.\"\n",
    "]\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "vectorizer = CountVectorizer(tokenizer)\n",
    "\n",
    "# fit_transform\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "print(f\"\\nVocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "for i, (doc, vec) in enumerate(zip(corpus, vectors)):\n",
    "    print(f\"  Doc {i}: {vec}  <- '{doc}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'and': 0, 'are': 1, 'cat': 2, 'cats': 3, 'dog': 4, 'dogs': 5, 'enemies': 6, 'log': 7, 'mat': 8, 'on': 9, 'sat': 10, 'the': 11}\n",
      "\n",
      "Document-Term Matrix:\n",
      "  Doc 0: [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 2]\n",
      "  Doc 1: [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 2]\n",
      "  Doc 2: [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "Verify 'the' (index=11): Doc0=2, Doc1=2\n"
     ]
    }
   ],
   "source": [
    "# Toy corpus khác để test thêm\n",
    "toy_corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "toy_vectorizer = CountVectorizer(RegexTokenizer())\n",
    "toy_vectors = toy_vectorizer.fit_transform(toy_corpus)\n",
    "\n",
    "print(\"Vocabulary:\", toy_vectorizer.vocabulary_)\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "for i, vec in enumerate(toy_vectors):\n",
    "    print(f\"  Doc {i}: {vec}\")\n",
    "\n",
    "# Verify: \"the\" appears twice in doc 0 and doc 1\n",
    "the_idx = toy_vectorizer.vocabulary_['the']\n",
    "print(f\"\\nVerify 'the' (index={the_idx}): Doc0={toy_vectors[0][the_idx]}, Doc1={toy_vectors[1][the_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test trên UD English EWT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Test Tokenizers trên EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12544 documents from EWT dataset.\n",
      "\n",
      "Sample documents:\n",
      "  [0]: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at t...\n",
      "  [1]: [ This killing of a respected cleric will be causing us trouble for years to com...\n",
      "  [2]: DPA : Iraqi authorities announced that they had busted up 3 terrorist cells oper...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_path = '../data/lab5/UD_English-EWT/en_ewt-ud-train.jsonl'\n",
    "documents = []\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        text = \" \".join(data['words'])\n",
    "        documents.append(text)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents from EWT dataset.\")\n",
    "print(f\"\\nSample documents:\")\n",
    "for i in range(min(3, len(documents))):\n",
    "    print(f\"  [{i}]: {documents[i][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer comparison on EWT samples:\n",
      "\n",
      "Doc 0: 'Al - Zaman : American forces killed Shaikh Abdulla...'\n",
      "  SimpleTokenizer: 29 tokens\n",
      "  RegexTokenizer:  23 tokens\n",
      "\n",
      "Doc 1: '[ This killing of a respected cleric will be causi...'\n",
      "  SimpleTokenizer: 18 tokens\n",
      "  RegexTokenizer:  15 tokens\n",
      "\n",
      "Doc 2: 'DPA : Iraqi authorities announced that they had bu...'\n",
      "  SimpleTokenizer: 17 tokens\n",
      "  RegexTokenizer:  15 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizers on EWT samples\n",
    "simple_tok = SimpleTokenizer()\n",
    "regex_tok = RegexTokenizer()\n",
    "\n",
    "print(\"Tokenizer comparison on EWT samples:\")\n",
    "for i in range(min(3, len(documents))):\n",
    "    doc = documents[i]\n",
    "    simple_tokens = simple_tok.tokenize(doc)\n",
    "    regex_tokens = regex_tok.tokenize(doc)\n",
    "    print(f\"\\nDoc {i}: '{doc[:50]}...'\")\n",
    "    print(f\"  SimpleTokenizer: {len(simple_tokens)} tokens\")\n",
    "    print(f\"  RegexTokenizer:  {len(regex_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test CountVectorizer trên EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15972\n",
      "\n",
      "Sample vocabulary (first 10):\n",
      "  '0': 0\n",
      "  '00': 1\n",
      "  '000': 2\n",
      "  '0000108806': 3\n",
      "  '0027': 4\n",
      "  '0046': 5\n",
      "  '008': 6\n",
      "  '01': 7\n",
      "  '0134': 8\n",
      "  '02': 9\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectorizer\n",
    "ewt_tokenizer = RegexTokenizer()\n",
    "ewt_vectorizer = CountVectorizer(ewt_tokenizer)\n",
    "\n",
    "# Fit on entire EWT dataset\n",
    "ewt_vectorizer.fit(documents)\n",
    "print(f\"Vocabulary size: {len(ewt_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Show some vocabulary samples\n",
    "vocab_items = list(ewt_vectorizer.vocabulary_.items())[:10]\n",
    "print(f\"\\nSample vocabulary (first 10):\")\n",
    "for word, idx in vocab_items:\n",
    "    print(f\"  '{word}': {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 document vectors:\n",
      "  Doc 0: length=15972, non-zero=19, total_count=23\n",
      "  Doc 1: length=15972, non-zero=15, total_count=15\n",
      "  Doc 2: length=15972, non-zero=15, total_count=15\n",
      "  Doc 3: length=15972, non-zero=12, total_count=15\n",
      "  Doc 4: length=15972, non-zero=30, total_count=34\n"
     ]
    }
   ],
   "source": [
    "# Transform first 5 documents\n",
    "ewt_vectors = ewt_vectorizer.transform(documents[:5])\n",
    "\n",
    "print(\"First 5 document vectors:\")\n",
    "for i, vec in enumerate(ewt_vectors):\n",
    "    non_zero = sum(1 for v in vec if v > 0)\n",
    "    total_count = sum(vec)\n",
    "    print(f\"  Doc {i}: length={len(vec)}, non-zero={non_zero}, total_count={total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity analysis (first 100 docs):\n",
      "  Total elements: 1597200\n",
      "  Non-zero elements: 1880\n",
      "  Sparsity: 0.9988 (99.88%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze sparsity\n",
    "sample_vectors = ewt_vectorizer.transform(documents[:100])\n",
    "total_elements = len(sample_vectors) * len(sample_vectors[0])\n",
    "non_zero_elements = sum(sum(1 for v in vec if v > 0) for vec in sample_vectors)\n",
    "sparsity = 1 - (non_zero_elements / total_elements)\n",
    "\n",
    "print(f\"Sparsity analysis (first 100 docs):\")\n",
    "print(f\"  Total elements: {total_elements}\")\n",
    "print(f\"  Non-zero elements: {non_zero_elements}\")\n",
    "print(f\"  Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. So sánh với Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison:\n",
      "  My Vocabulary Size:      15972\n",
      "  Sklearn Vocabulary Size: 15936\n",
      "\n",
      "  My Vector Shape:      (12544, 15972)\n",
      "  Sklearn Vector Shape: (12544, 15936)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as SklearnCountVectorizer\n",
    "\n",
    "# Sklearn vectorizer\n",
    "sklearn_vectorizer = SklearnCountVectorizer()\n",
    "sklearn_vectors = sklearn_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(f\"  My Vocabulary Size:      {len(ewt_vectorizer.vocabulary_)}\")\n",
    "print(f\"  Sklearn Vocabulary Size: {len(sklearn_vectorizer.vocabulary_)}\")\n",
    "print(f\"\\n  My Vector Shape:      ({len(documents)}, {len(ewt_vectorizer.vocabulary_)})\")\n",
    "print(f\"  Sklearn Vector Shape: {sklearn_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My vocabulary: {'a': 0, 'ai': 1, 'i': 2, 'is': 3, 'love': 4, 'nlp': 5, 'of': 6, 'programming': 7, 'subfield': 8}\n",
      "Sklearn vocabulary: {'ai': 0, 'is': 1, 'love': 2, 'nlp': 3, 'of': 4, 'programming': 5, 'subfield': 6}\n",
      "\n",
      "My vectors:\n",
      "  [0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "  [0, 0, 1, 0, 1, 0, 0, 1, 0]\n",
      "  [1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "\n",
      "Sklearn vectors:\n",
      "  [np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "  [np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "  [np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# Compare on toy corpus to verify correctness\n",
    "test_corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"I love programming.\",\n",
    "    \"NLP is a subfield of AI.\"\n",
    "]\n",
    "\n",
    "my_vec = CountVectorizer(RegexTokenizer())\n",
    "my_result = my_vec.fit_transform(test_corpus)\n",
    "\n",
    "sk_vec = SklearnCountVectorizer(lowercase=True)\n",
    "sk_result = sk_vec.fit_transform(test_corpus).toarray()\n",
    "\n",
    "print(\"My vocabulary:\", my_vec.vocabulary_)\n",
    "print(\"Sklearn vocabulary:\", dict(sorted(sk_vec.vocabulary_.items(), key=lambda x: x[1])))\n",
    "print(\"\\nMy vectors:\")\n",
    "for v in my_result:\n",
    "    print(f\"  {v}\")\n",
    "print(\"\\nSklearn vectors:\")\n",
    "for v in sk_result:\n",
    "    print(f\"  {list(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giải thích sự khác biệt:\n",
    "- **Tokenization khác nhau**: Sklearn mặc định dùng regex `(?u)\\b\\w\\w+\\b` (từ >= 2 ký tự), còn ta dùng `\\b\\w+\\b` (từ >= 1 ký tự)\n",
    "- **Vocabulary size**: Sklearn có thể lớn hơn/nhỏ hơn tùy cách xử lý punctuation và single-character tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
