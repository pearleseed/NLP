{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Lab 5: RNNs for Text and Token Classification\n",
        "\n",
        "**Mục tiêu:** Tìm hiểu và ứng dụng Mạng Nơ-ron Hồi quy (RNN/LSTM) cho các bài toán phân loại văn bản và phân loại token.\n",
        "\n",
        "**Nội dung:**\n",
        "1. Part 1: Làm quen với PyTorch (15%)\n",
        "2. Part 2: Phân loại văn bản với RNN/LSTM (35%)\n",
        "3. Part 3: Part-of-Speech Tagging với RNN (25%)\n",
        "4. Part 4: Named Entity Recognition với RNN (25%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Import thư viện cần thiết\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device('mps')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 1: Làm quen với PyTorch\n",
        "\n",
        "Tensor là cấu trúc dữ liệu cốt lõi của PyTorch, tương tự ndarray của NumPy nhưng có thêm khả năng chạy trên GPU và tự động tính đạo hàm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Task 1.1: Tạo Tensor ---\n",
            "Tensor từ list:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "\n",
            "Tensor từ NumPy array:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "\n",
            "Ones Tensor:\n",
            " tensor([[1, 1],\n",
            "        [1, 1]])\n",
            "\n",
            "Random Tensor:\n",
            " tensor([[0.7358, 0.4764],\n",
            "        [0.3339, 0.4610]])\n",
            "\n",
            "Shape của tensor: torch.Size([2, 2])\n",
            "Datatype của tensor: torch.float32\n",
            "Device lưu trữ tensor: cpu\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Task 1.1: Tạo Tensor ---\")\n",
        "# Tạo tensor từ list\n",
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "print(f\"Tensor từ list:\\n {x_data}\\n\")\n",
        "\n",
        "# Tạo tensor từ NumPy array\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print(f\"Tensor từ NumPy array:\\n {x_np}\\n\")\n",
        "\n",
        "# Tạo tensor với các giá trị ngẫu nhiên hoặc hằng số\n",
        "x_ones = torch.ones_like(x_data)\n",
        "print(f\"Ones Tensor:\\n {x_ones}\\n\")\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "print(f\"Random Tensor:\\n {x_rand}\\n\")\n",
        "\n",
        "# In ra shape, dtype, và device của tensor\n",
        "print(f\"Shape của tensor: {x_rand.shape}\")\n",
        "print(f\"Datatype của tensor: {x_rand.dtype}\")\n",
        "print(f\"Device lưu trữ tensor: {x_rand.device}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Task 1.2: Các phép toán trên Tensor ---\n",
            "x_data + x_data:\n",
            " tensor([[2, 4],\n",
            "        [6, 8]])\n",
            "\n",
            "x_data * 5:\n",
            " tensor([[ 5, 10],\n",
            "        [15, 20]])\n",
            "\n",
            "x_data @ x_data.T:\n",
            " tensor([[ 5, 11],\n",
            "        [11, 25]])\n",
            "\n",
            "--- Task 1.3: Indexing và Slicing ---\n",
            "Hàng đầu tiên: tensor([1, 2])\n",
            "Cột thứ hai: tensor([2, 4])\n",
            "Giá trị [1, 1]: 4\n",
            "\n",
            "--- Task 1.4: Thay đổi hình dạng Tensor ---\n",
            "Tensor gốc (4, 4):\n",
            " tensor([[0.2440, 0.9613, 0.3792, 0.7411],\n",
            "        [0.8852, 0.7397, 0.4502, 0.8711],\n",
            "        [0.0735, 0.2276, 0.7354, 0.0615],\n",
            "        [0.3618, 0.5140, 0.3774, 0.3569]])\n",
            "\n",
            "Tensor đã reshape (16, 1):\n",
            " tensor([[0.2440],\n",
            "        [0.9613],\n",
            "        [0.3792],\n",
            "        [0.7411],\n",
            "        [0.8852],\n",
            "        [0.7397],\n",
            "        [0.4502],\n",
            "        [0.8711],\n",
            "        [0.0735],\n",
            "        [0.2276],\n",
            "        [0.7354],\n",
            "        [0.0615],\n",
            "        [0.3618],\n",
            "        [0.5140],\n",
            "        [0.3774],\n",
            "        [0.3569]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Task 1.2: Các phép toán trên Tensor ---\")\n",
        "print(f\"x_data + x_data:\\n {x_data + x_data}\\n\")\n",
        "print(f\"x_data * 5:\\n {x_data * 5}\\n\")\n",
        "print(f\"x_data @ x_data.T:\\n {x_data @ x_data.T}\\n\")\n",
        "\n",
        "print(\"--- Task 1.3: Indexing và Slicing ---\")\n",
        "print(f\"Hàng đầu tiên: {x_data[0]}\")\n",
        "print(f\"Cột thứ hai: {x_data[:, 1]}\")\n",
        "print(f\"Giá trị [1, 1]: {x_data[1, 1]}\\n\")\n",
        "\n",
        "print(\"--- Task 1.4: Thay đổi hình dạng Tensor ---\")\n",
        "rand_tensor = torch.rand(4, 4)\n",
        "print(f\"Tensor gốc (4, 4):\\n {rand_tensor}\\n\")\n",
        "reshaped_tensor = rand_tensor.view(16, 1)\n",
        "print(f\"Tensor đã reshape (16, 1):\\n {reshaped_tensor}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Phần 2: Tự động tính Đạo hàm với autograd ---\n",
            "Đạo hàm dz/dx khi x=1: tensor([18.])\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Phần 2: Tự động tính Đạo hàm với autograd ---\")\n",
        "x = torch.ones(1, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "z.backward()\n",
        "print(f\"Đạo hàm dz/dx khi x=1: {x.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Câu hỏi:** Chuyện gì xảy ra nếu bạn gọi z.backward() một lần nữa? Tại sao?\n",
        "\n",
        "> **Trả lời:** Sẽ xảy ra lỗi 'RuntimeError: Trying to backward through the graph a second time...'. Theo mặc định, PyTorch sẽ xóa đồ thị tính toán (computation graph) ngay sau khi thực hiện lan truyền ngược (.backward()) để tiết kiệm bộ nhớ. Nếu muốn tính đạo hàm lại, ta cần chỉ định `retain_graph=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Phần 3: Xây dựng Mô hình với torch.nn ---\n",
            "Model input shape: torch.Size([1, 4])\n",
            "Model output shape: torch.Size([1, 2])\n",
            "Model output:\n",
            " tensor([[-0.0162,  0.1156]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Phần 3: Xây dựng Mô hình với torch.nn ---\")\n",
        "\n",
        "class MyFirstModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(MyFirstModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, indices):\n",
        "        embeds = self.embedding(indices)\n",
        "        pooled = embeds.mean(dim=1)\n",
        "        hidden = self.activation(self.linear(pooled))\n",
        "        output = self.output_layer(hidden)\n",
        "        return output\n",
        "\n",
        "model = MyFirstModel(vocab_size=100, embedding_dim=16, hidden_dim=8, output_dim=2)\n",
        "input_data = torch.LongTensor([[1, 2, 5, 9]])\n",
        "output_data = model(input_data)\n",
        "print(f\"Model input shape: {input_data.shape}\")\n",
        "print(f\"Model output shape: {output_data.shape}\")\n",
        "print(f\"Model output:\\n {output_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 2: Phân loại văn bản với RNN/LSTM\n",
        "\n",
        "Sử dụng bộ dữ liệu HWU64 để phân loại ý định người dùng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 8954, Val: 1076, Test: 1076\n",
            "Số lượng intent: 64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# Tải dữ liệu HWU\n",
        "df_train = pd.read_csv('../data/lab5/HWU/train.csv')\n",
        "df_val = pd.read_csv('../data/lab5/HWU/val.csv')\n",
        "df_test = pd.read_csv('../data/lab5/HWU/test.csv')\n",
        "\n",
        "# Tiền xử lý nhãn\n",
        "label_encoder = LabelEncoder()\n",
        "df_train['intent_encoded'] = label_encoder.fit_transform(df_train['category'])\n",
        "df_val['intent_encoded'] = label_encoder.transform(df_val['category'])\n",
        "df_test['intent_encoded'] = label_encoder.transform(df_test['category'])\n",
        "\n",
        "X_train, y_train = df_train['text'], df_train['intent_encoded']\n",
        "X_val, y_val = df_val['text'], df_val['intent_encoded']\n",
        "X_test, y_test = df_test['text'], df_test['intent_encoded']\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "print(f\"Số lượng intent: {len(label_encoder.classes_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Baseline 1 (TF-IDF + Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Kết quả TF-IDF + Logistic Regression ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92        19\n",
            "           1       1.00      0.73      0.84        11\n",
            "           2       0.77      0.89      0.83        19\n",
            "           3       1.00      0.75      0.86         8\n",
            "           4       0.92      0.80      0.86        15\n",
            "           5       0.93      1.00      0.96        13\n",
            "           6       0.45      0.53      0.49        19\n",
            "           7       0.89      0.89      0.89        19\n",
            "           8       0.87      0.68      0.76        19\n",
            "           9       0.59      0.68      0.63        19\n",
            "          10       0.67      0.75      0.71         8\n",
            "          11       0.74      0.89      0.81        19\n",
            "          12       0.78      0.88      0.82         8\n",
            "          13       0.83      0.79      0.81        19\n",
            "          14       0.92      0.63      0.75        19\n",
            "          15       0.81      0.89      0.85        19\n",
            "          16       1.00      1.00      1.00        19\n",
            "          17       1.00      1.00      1.00        19\n",
            "          18       1.00      1.00      1.00        19\n",
            "          19       0.90      1.00      0.95        19\n",
            "          20       1.00      0.95      0.97        19\n",
            "          21       1.00      1.00      1.00        12\n",
            "          22       0.95      1.00      0.97        19\n",
            "          23       0.95      1.00      0.97        19\n",
            "          24       0.36      0.26      0.30        19\n",
            "          25       0.90      1.00      0.95        19\n",
            "          26       1.00      1.00      1.00        16\n",
            "          27       1.00      0.95      0.97        19\n",
            "          28       0.75      0.79      0.77        19\n",
            "          29       0.91      0.83      0.87        12\n",
            "          30       0.89      0.89      0.89        19\n",
            "          31       0.67      0.67      0.67         3\n",
            "          32       1.00      0.86      0.92        14\n",
            "          33       0.80      0.89      0.84         9\n",
            "          34       0.78      1.00      0.88         7\n",
            "          35       0.68      0.79      0.73        19\n",
            "          36       0.75      0.79      0.77        19\n",
            "          37       0.85      0.89      0.87        19\n",
            "          38       0.65      0.61      0.63        18\n",
            "          39       0.71      0.53      0.61        19\n",
            "          40       1.00      0.57      0.73         7\n",
            "          41       0.75      0.63      0.69        19\n",
            "          42       0.95      0.95      0.95        19\n",
            "          43       0.81      0.68      0.74        19\n",
            "          44       0.58      0.74      0.65        19\n",
            "          45       1.00      0.84      0.91        19\n",
            "          46       0.89      0.84      0.86        19\n",
            "          47       0.94      0.89      0.92        19\n",
            "          48       0.82      0.95      0.88        19\n",
            "          49       0.48      0.58      0.52        19\n",
            "          50       0.92      0.86      0.89        14\n",
            "          51       1.00      0.95      0.97        19\n",
            "          52       0.83      0.79      0.81        19\n",
            "          53       0.81      0.89      0.85        19\n",
            "          54       1.00      1.00      1.00        10\n",
            "          55       0.95      1.00      0.97        19\n",
            "          56       0.80      0.89      0.84        18\n",
            "          57       0.83      0.79      0.81        19\n",
            "          58       0.89      0.89      0.89        19\n",
            "          59       0.68      0.79      0.73        19\n",
            "          60       1.00      1.00      1.00        18\n",
            "          61       0.94      0.79      0.86        19\n",
            "          62       1.00      0.95      0.97        19\n",
            "          63       0.62      0.68      0.65        19\n",
            "\n",
            "    accuracy                           0.84      1076\n",
            "   macro avg       0.85      0.83      0.84      1076\n",
            "weighted avg       0.84      0.84      0.84      1076\n",
            "\n",
            "Macro F1-score: 0.8353\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "tfidf_lr_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=5000),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "tfidf_lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tfidf = tfidf_lr_pipeline.predict(X_test)\n",
        "print(\"--- Kết quả TF-IDF + Logistic Regression ---\")\n",
        "print(classification_report(y_test, y_pred_tfidf, zero_division=0))\n",
        "f1_tfidf = f1_score(y_test, y_pred_tfidf, average='macro')\n",
        "print(f\"Macro F1-score: {f1_tfidf:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Baseline 2 (Word2Vec Trung bình + Dense Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors, Word2Vec\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import os\n",
        "\n",
        "# Load GloVe from Text File\n",
        "glove_input_file = '../data/glove.6B.50d.txt'\n",
        "word2vec_output_file = '../data/glove.6B.50d.txt.word2vec'\n",
        "\n",
        "if not os.path.exists(word2vec_output_file):\n",
        "    print(\"Converting GloVe to Word2Vec format (this may take a moment)...\")\n",
        "    glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "print(f\"Loading GloVe vectors from {word2vec_output_file}...\")\n",
        "w2v_vectors = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
        "print(\"GloVe model loaded successfully! Vector size:\", w2v_vectors.vector_size)\n",
        "\n",
        "def sentence_to_avg_vector(text, vectors):\n",
        "    words = text.split()\n",
        "    # Use 'vectors' directly as KeyedVectors\n",
        "    word_vectors = [vectors[word] for word in words if word in vectors]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(vectors.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Pass w2v_vectors instead of w2v_model\n",
        "X_train_avg = np.array([sentence_to_avg_vector(text, w2v_vectors) for text in X_train])\n",
        "X_val_avg = np.array([sentence_to_avg_vector(text, w2v_vectors) for text in X_val])\n",
        "X_test_avg = np.array([sentence_to_avg_vector(text, w2v_vectors) for text in X_test])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "w2v_dense_model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(w2v_vectors.vector_size,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "w2v_dense_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "w2v_dense_model.fit(X_train_avg, y_train, epochs=10, validation_data=(X_val_avg, y_val), batch_size=32, verbose=0)\n",
        "\n",
        "y_pred_w2v = np.argmax(w2v_dense_model.predict(X_test_avg), axis=1)\n",
        "print(\"--- Kết quả Word2Vec (Avg) + Dense Layer ---\")\n",
        "print(classification_report(y_test, y_pred_w2v, zero_division=0))\n",
        "f1_w2v = f1_score(y_test, y_pred_w2v, average='macro')\n",
        "print(f\"Macro F1-score: {f1_w2v:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3 & 4: Mô hình LSTM (Pre-trained vs Scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Tiền xử lý cho mô hình chuỗi\n",
        "vocab_size = 10000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<UNK>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "max_len = 50\n",
        "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding='post')\n",
        "X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n",
        "\n",
        "# Tạo ma trận embedding từ GloVe\n",
        "embedding_dim = w2v_vectors.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size and word in w2v_vectors:\n",
        "        embedding_matrix[i] = w2v_vectors[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM với Pre-trained Embeddings\n",
        "lstm_pretrained = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "lstm_pretrained.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "print(\"--- Huấn luyện LSTM (Pre-trained) ---\")\n",
        "lstm_pretrained.fit(X_train_pad, y_train, epochs=15, validation_data=(X_val_pad, y_val), \n",
        "                    batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "y_pred_lstm_pre = np.argmax(lstm_pretrained.predict(X_test_pad), axis=1)\n",
        "f1_lstm_pre = f1_score(y_test, y_pred_lstm_pre, average='macro')\n",
        "print(f\"LSTM Pre-trained Macro F1: {f1_lstm_pre:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM học từ đầu (Scratch)\n",
        "lstm_scratch = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=max_len, trainable=True),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "lstm_scratch.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--- Huấn luyện LSTM (Scratch) ---\")\n",
        "lstm_scratch.fit(X_train_pad, y_train, epochs=15, validation_data=(X_val_pad, y_val),\n",
        "                 batch_size=32, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "y_pred_lstm_scratch = np.argmax(lstm_scratch.predict(X_test_pad), axis=1)\n",
        "f1_lstm_scratch = f1_score(y_test, y_pred_lstm_scratch, average='macro')\n",
        "print(f\"LSTM Scratch Macro F1: {f1_lstm_scratch:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 5: So sánh và Phân tích"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== SO SÁNH KẾT QUẢ ===\")\n",
        "print(f\"TF-IDF + LR:        F1 = {f1_tfidf:.4f}\")\n",
        "print(f\"Word2Vec + Dense:   F1 = {f1_w2v:.4f}\")\n",
        "print(f\"LSTM Pre-trained:   F1 = {f1_lstm_pre:.4f}\")\n",
        "print(f\"LSTM Scratch:       F1 = {f1_lstm_scratch:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Phân tích:**\n",
        "- **TF-IDF + LR**: Baseline mạnh, nhanh, nhưng bỏ qua thứ tự từ.\n",
        "- **Word2Vec + Dense**: Kết quả kém do mất thông tin ngữ cảnh khi lấy trung bình.\n",
        "- **LSTM**: Hiểu được ngữ cảnh và thứ tự từ, đặc biệt hiệu quả với câu có phủ định.\n",
        "\n",
        "**Ví dụ minh họa:**\n",
        "- Câu: \"can you remind me to not call my mom\" (intent: reminder_create)\n",
        "- Baseline có thể nhầm do từ \"call\", nhưng LSTM hiểu \"not\" phủ định hành động."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 3: Part-of-Speech Tagging với RNN\n",
        "\n",
        "Sử dụng bộ dữ liệu UD_English-EWT để gán nhãn từ loại."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_jsonl(file_path):\n",
        "    sentences = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line.strip())\n",
        "            sentence = list(zip(data['words'], data['tags']))\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "train_sents = load_jsonl('../data/lab5/UD_English-EWT/en_ewt-ud-train.jsonl')\n",
        "dev_sents = load_jsonl('../data/lab5/UD_English-EWT/en_ewt-ud-dev.jsonl')\n",
        "\n",
        "# Xây dựng từ điển\n",
        "word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
        "tag_to_ix = {'<PAD>': 0}\n",
        "\n",
        "for sent in train_sents:\n",
        "    for word, tag in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "\n",
        "print(f\"Kích thước từ điển từ: {len(word_to_ix)}\")\n",
        "print(f\"Kích thước từ điển nhãn: {len(tag_to_ix)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POSDataset(Dataset):\n",
        "    def __init__(self, sents, word_to_ix, tag_to_ix):\n",
        "        self.sents_encoded = []\n",
        "        self.tags_encoded = []\n",
        "        \n",
        "        for sent in sents:\n",
        "             # Pre-encode words and tags during init\n",
        "            word_indices = [word_to_ix.get(w, word_to_ix['<UNK>']) for w, t in sent]\n",
        "            tag_indices = [tag_to_ix.get(t) for w, t in sent]\n",
        "            self.sents_encoded.append(torch.tensor(word_indices))\n",
        "            self.tags_encoded.append(torch.tensor(tag_indices))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Simply return the pre-encoded tensors\n",
        "        return self.sents_encoded[idx], self.tags_encoded[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_to_ix['<PAD>'])\n",
        "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag_to_ix['<PAD>'])\n",
        "    return sentences_padded, tags_padded\n",
        "\n",
        "train_dataset = POSDataset(train_sents, word_to_ix, tag_to_ix)\n",
        "dev_dataset = POSDataset(dev_sents, word_to_ix, tag_to_ix)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRNNForPOS(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embedding(sentence)\n",
        "        rnn_out, _ = self.rnn(embeds)\n",
        "        tag_scores = self.linear(rnn_out)\n",
        "        return tag_scores\n",
        "\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 64\n",
        "pos_model = SimpleRNNForPOS(len(word_to_ix), EMBEDDING_DIM, HIDDEN_DIM, len(tag_to_ix)).to(device)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=tag_to_ix['<PAD>'])\n",
        "optimizer = optim.Adam(pos_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Huấn luyện\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    pos_model.train()\n",
        "    total_loss = 0\n",
        "    for sentence_in, tags_in in train_loader:\n",
        "        sentence_in, tags_in = sentence_in.to(device), tags_in.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        tag_scores = pos_model(sentence_in)\n",
        "        loss = loss_function(tag_scores.view(-1, len(tag_to_ix)), tags_in.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pos(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sentence_in, tags_in in loader:\n",
        "            sentence_in, tags_in = sentence_in.to(device), tags_in.to(device)\n",
        "            tag_scores = model(sentence_in)\n",
        "            preds = torch.argmax(tag_scores, dim=2)\n",
        "            mask = (tags_in != tag_to_ix['<PAD>'])\n",
        "            correct += (preds[mask] == tags_in[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "    return correct / total * 100 if total > 0 else 0\n",
        "\n",
        "def predict_pos(sentence_str):\n",
        "    pos_model.eval()\n",
        "    words = sentence_str.split()\n",
        "    word_indices = [word_to_ix.get(w, word_to_ix['<UNK>']) for w in words]\n",
        "    sent_tensor = torch.LongTensor([word_indices]).to(device)\n",
        "    with torch.no_grad():\n",
        "        tag_scores = pos_model(sent_tensor)\n",
        "        pred_indices = torch.argmax(tag_scores, dim=2).squeeze(0).tolist()\n",
        "    ix_to_tag = {i: t for t, i in tag_to_ix.items()}\n",
        "    return ' '.join([f\"{w}/{ix_to_tag[i]}\" for w, i in zip(words, pred_indices)])\n",
        "\n",
        "print(f\"\\nĐộ chính xác trên tập dev: {evaluate_pos(pos_model, dev_loader):.2f}%\")\n",
        "print(f\"\\nVí dụ: {predict_pos('I love NLP')}\")\n",
        "print(f\"Ví dụ: {predict_pos('From the AP comes this story')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part 4: Named Entity Recognition với RNN\n",
        "\n",
        "Sử dụng bộ dữ liệu CoNLL-2003 để nhận dạng thực thể tên."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Tải dữ liệu CoNLL-2003\n",
        "dataset = load_dataset('conll2003', trust_remote_code=True)\n",
        "\n",
        "# Lấy ánh xạ nhãn\n",
        "ner_labels = dataset['train'].features['ner_tags'].feature.names\n",
        "print(f\"NER Labels: {ner_labels}\")\n",
        "\n",
        "# Trích xuất dữ liệu\n",
        "train_tokens = dataset['train']['tokens']\n",
        "train_ner_tags = [[ner_labels[t] for t in tags] for tags in dataset['train']['ner_tags']]\n",
        "val_tokens = dataset['validation']['tokens']\n",
        "val_ner_tags = [[ner_labels[t] for t in tags] for tags in dataset['validation']['ner_tags']]\n",
        "\n",
        "print(f\"Train: {len(train_tokens)} câu, Val: {len(val_tokens)} câu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Xây dựng từ điển cho NER\n",
        "ner_word_to_ix = {'<PAD>': 0, '<UNK>': 1}\n",
        "ner_tag_to_ix = {'<PAD>': 0}\n",
        "\n",
        "for tokens in train_tokens:\n",
        "    for word in tokens:\n",
        "        if word not in ner_word_to_ix:\n",
        "            ner_word_to_ix[word] = len(ner_word_to_ix)\n",
        "\n",
        "for tag in ner_labels:\n",
        "    if tag not in ner_tag_to_ix:\n",
        "        ner_tag_to_ix[tag] = len(ner_tag_to_ix)\n",
        "\n",
        "print(f\"Vocab size: {len(ner_word_to_ix)}, Tag size: {len(ner_tag_to_ix)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokens_list, tags_list, word_to_ix, tag_to_ix):\n",
        "        self.sentences_encoded = []\n",
        "        self.tags_encoded = []\n",
        "        \n",
        "        for tokens, tags in zip(tokens_list, tags_list):\n",
        "            word_indices = [word_to_ix.get(w, word_to_ix['<UNK>']) for w in tokens]\n",
        "            tag_indices = [tag_to_ix.get(t, 0) for t in tags]\n",
        "            \n",
        "            self.sentences_encoded.append(torch.tensor(word_indices))\n",
        "            self.tags_encoded.append(torch.tensor(tag_indices))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences_encoded[idx], self.tags_encoded[idx]\n",
        "\n",
        "def ner_collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=ner_word_to_ix['<PAD>'])\n",
        "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=ner_tag_to_ix['<PAD>'])\n",
        "    return sentences_padded, tags_padded\n",
        "\n",
        "ner_train_dataset = NERDataset(train_tokens, train_ner_tags, ner_word_to_ix, ner_tag_to_ix)\n",
        "ner_val_dataset = NERDataset(val_tokens, val_ner_tags, ner_word_to_ix, ner_tag_to_ix)\n",
        "ner_train_loader = DataLoader(ner_train_dataset, batch_size=32, shuffle=True, collate_fn=ner_collate_fn)\n",
        "ner_val_loader = DataLoader(ner_val_dataset, batch_size=32, collate_fn=ner_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiLSTMForNER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_scores = self.linear(lstm_out)\n",
        "        return tag_scores\n",
        "\n",
        "NER_EMBEDDING_DIM = 100\n",
        "NER_HIDDEN_DIM = 128\n",
        "ner_model = BiLSTMForNER(len(ner_word_to_ix), NER_EMBEDDING_DIM, NER_HIDDEN_DIM, len(ner_tag_to_ix)).to(device)\n",
        "ner_loss_fn = nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix['<PAD>'])\n",
        "ner_optimizer = optim.Adam(ner_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Huấn luyện NER\n",
        "NER_EPOCHS = 5\n",
        "for epoch in range(NER_EPOCHS):\n",
        "    ner_model.train()\n",
        "    total_loss = 0\n",
        "    for sentence_in, tags_in in ner_train_loader:\n",
        "        sentence_in, tags_in = sentence_in.to(device), tags_in.to(device)\n",
        "        ner_optimizer.zero_grad()\n",
        "        tag_scores = ner_model(sentence_in)\n",
        "        loss = ner_loss_fn(tag_scores.view(-1, len(ner_tag_to_ix)), tags_in.view(-1))\n",
        "        loss.backward()\n",
        "        ner_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{NER_EPOCHS}, Loss: {total_loss / len(ner_train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_ner(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sentence_in, tags_in in loader:\n",
        "            sentence_in, tags_in = sentence_in.to(device), tags_in.to(device)\n",
        "            tag_scores = model(sentence_in)\n",
        "            preds = torch.argmax(tag_scores, dim=2)\n",
        "            mask = (tags_in != ner_tag_to_ix['<PAD>'])\n",
        "            correct += (preds[mask] == tags_in[mask]).sum().item()\n",
        "            total += mask.sum().item()\n",
        "    return correct / total * 100 if total > 0 else 0\n",
        "\n",
        "def predict_ner(sentence_str):\n",
        "    ner_model.eval()\n",
        "    words = sentence_str.split()\n",
        "    word_indices = [ner_word_to_ix.get(w, ner_word_to_ix['<UNK>']) for w in words]\n",
        "    sent_tensor = torch.LongTensor([word_indices]).to(device)\n",
        "    with torch.no_grad():\n",
        "        tag_scores = ner_model(sent_tensor)\n",
        "        pred_indices = torch.argmax(tag_scores, dim=2).squeeze(0).tolist()\n",
        "    ix_to_tag = {i: t for t, i in ner_tag_to_ix.items()}\n",
        "    return ' '.join([f\"{w}/{ix_to_tag[i]}\" for w, i in zip(words, pred_indices)])\n",
        "\n",
        "print(f\"\\nĐộ chính xác NER trên tập validation: {evaluate_ner(ner_model, ner_val_loader):.2f}%\")\n",
        "print(f\"\\nVí dụ: {predict_ner('John lives in New York')}\")\n",
        "print(f\"Ví dụ: {predict_ner('Apple Inc is based in California')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
