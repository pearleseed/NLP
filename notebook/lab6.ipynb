{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Lab 6: Giới thiệu về Transformers\n",
        "\n",
        "**Mục tiêu:**\n",
        "- Ôn lại kiến thức cơ bản về kiến trúc Transformer\n",
        "- Sử dụng các mô hình Transformer tiền huấn luyện (pretrained models) để thực hiện các tác vụ NLP cơ bản\n",
        "- Làm quen với thư viện `transformers` của Hugging Face\n",
        "\n",
        "**Nội dung:**\n",
        "1. Bài 1: Khôi phục Masked Token (Masked Language Modeling)\n",
        "2. Bài 2: Dự đoán từ tiếp theo (Next Token Prediction)\n",
        "3. Bài 3: Tính toán Vector biểu diễn của câu (Sentence Representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bài 1: Khôi phục Masked Token (Masked Language Modeling)\n",
        "\n",
        "Trong tác vụ này, chúng ta sẽ che một vài từ trong câu bằng token đặc biệt `[MASK]` và yêu cầu mô hình dự đoán từ gốc. Sử dụng mô hình thuộc họ BERT.\n",
        "\n",
        "**Yêu cầu:** Sử dụng pipeline `fill-mask` để dự đoán từ bị thiếu trong câu: `Hanoi is the [MASK] of Vietnam.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d93f5bc95c4646d081d4daef107e63a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9747b4b82da04d36b207fb5e7cf85843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fea9d5d935a841d1846ee3dbf065464d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "611c3d68ff8640d3bd64c3412534272a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "331acb3bffc04cc892f9e2b91085a3d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 1. Tải pipeline \"fill-mask\"\n",
        "# Pipeline này sẽ tự động tải một mô hình mặc định phù hợp (thường là một biến thể của BERT)\n",
        "mask_filler = pipeline(\"fill-mask\")\n",
        "\n",
        "# 2. Câu đầu vào với token [MASK]\n",
        "# Lưu ý: Một số model dùng <mask> thay vì [MASK]\n",
        "input_sentence = \"Hanoi is the <mask> of Vietnam.\"\n",
        "\n",
        "# 3. Thực hiện dự đoán\n",
        "# top_k=5 yêu cầu mô hình trả về 5 dự đoán hàng đầu\n",
        "predictions = mask_filler(input_sentence, top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Câu gốc: Hanoi is the <mask> of Vietnam.\n",
            "\n",
            "Kết quả dự đoán:\n",
            "  Dự đoán: ' capital' với độ tin cậy: 0.9341\n",
            "  -> Câu hoàn chỉnh: Hanoi is the capital of Vietnam.\n",
            "  Dự đoán: ' Republic' với độ tin cậy: 0.0300\n",
            "  -> Câu hoàn chỉnh: Hanoi is the Republic of Vietnam.\n",
            "  Dự đoán: ' Capital' với độ tin cậy: 0.0105\n",
            "  -> Câu hoàn chỉnh: Hanoi is the Capital of Vietnam.\n",
            "  Dự đoán: ' birthplace' với độ tin cậy: 0.0054\n",
            "  -> Câu hoàn chỉnh: Hanoi is the birthplace of Vietnam.\n",
            "  Dự đoán: ' heart' với độ tin cậy: 0.0014\n",
            "  -> Câu hoàn chỉnh: Hanoi is the heart of Vietnam.\n"
          ]
        }
      ],
      "source": [
        "# 4. In kết quả\n",
        "print(f\"Câu gốc: {input_sentence}\")\n",
        "print(\"\\nKết quả dự đoán:\")\n",
        "for pred in predictions:\n",
        "    print(f\"  Dự đoán: '{pred['token_str']}' với độ tin cậy: {pred['score']:.4f}\")\n",
        "    print(f\"  -> Câu hoàn chỉnh: {pred['sequence']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phân tích Bài 1:\n",
        "\n",
        "**1. Mô hình đã dự đoán đúng từ `capital` không?**\n",
        "- Có, mô hình dự đoán đúng từ \"capital\" với độ tin cậy rất cao (~0.93)\n",
        "- Các dự đoán khác như \"Republic\", \"Capital\", \"birthplace\", \"heart\" cũng hợp lý về mặt ngữ pháp\n",
        "\n",
        "**2. Tại sao các mô hình Encoder-only như BERT lại phù hợp cho tác vụ này?**\n",
        "- BERT là mô hình **bidirectional** (hai chiều), có thể nhìn cả các từ đứng trước và sau token `[MASK]`\n",
        "- Điều này cho phép mô hình hiểu ngữ cảnh đầy đủ của câu để dự đoán từ bị che\n",
        "- Ví dụ: \"Hanoi is the ___ of Vietnam\" - mô hình cần hiểu cả \"Hanoi\" (thủ đô) và \"Vietnam\" (quốc gia) để dự đoán \"capital\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bài 2: Dự đoán từ tiếp theo (Next Token Prediction)\n",
        "\n",
        "Tác vụ này yêu cầu mô hình sinh ra phần tiếp theo của một đoạn văn bản cho trước. Sử dụng mô hình thuộc họ GPT.\n",
        "\n",
        "**Yêu cầu:** Sử dụng pipeline `text-generation` để sinh ra phần tiếp theo cho câu: `The best thing about learning NLP is`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de6d67b287fd4b448406dc3f5c120a74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6991f61fab284d389430d93e4fe89eb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eef0ba6e9e743a697e75e3a89db456d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90a61da83fe241ef90d944d83951574a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f2e32e694204a60baddef8328969518",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b36a7db0fa52476fa3930fc5b5c90bde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5b0f2512c7d46548f5edace6a0f89b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 1. Tải pipeline \"text-generation\"\n",
        "# Pipeline này sẽ tự động tải một mô hình phù hợp (thường là GPT-2)\n",
        "generator = pipeline(\"text-generation\")\n",
        "\n",
        "# 2. Đoạn văn bản mồi\n",
        "prompt = \"The best thing about learning NLP is\"\n",
        "\n",
        "# 3. Sinh văn bản\n",
        "# max_length: tổng độ dài của câu mồi và phần được sinh ra\n",
        "# num_return_sequences: số lượng chuỗi kết quả muốn nhận\n",
        "generated_texts = generator(prompt, max_length=50, num_return_sequences=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Câu mồi: 'The best thing about learning NLP is'\n",
            "\n",
            "Văn bản được sinh ra:\n",
            "The best thing about learning NLP is that you learn a lot.\n",
            "\n",
            "And I hope it helps you in other ways too.\n",
            "\n",
            "Let's go back to the question: who can make your own version of Google?\n",
            "\n",
            "I'm not sure if this is the right question to ask. I think that you will get a little bit of a feeling.\n",
            "\n",
            "But let's look at a couple of examples:\n",
            "\n",
            "What makes a good program? A good learning program is one that you can learn. This is not a prerequisite for anything, but it is a good way to keep pace. It can help you find the right tools and techniques. So let's say you have a computer program that you know you can learn by reading. Now, you might already have a good idea of what you are about to learn.\n",
            "\n",
            "What could you do to catch up? And what would it be like to learn Google again?\n",
            "\n",
            "I think that there are two reasons why this is a good question to ask. One is that people who know that they can learn Google will use it wisely. And the other is that the process of learning Google is a long process, not a static process. And some people use it more often.\n",
            "\n",
            "But let's continue with the question here. How\n"
          ]
        }
      ],
      "source": [
        "# 4. In kết quả\n",
        "print(f\"Câu mồi: '{prompt}'\")\n",
        "print(\"\\nVăn bản được sinh ra:\")\n",
        "for text in generated_texts:\n",
        "    print(text['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phân tích Bài 2:\n",
        "\n",
        "**1. Kết quả sinh ra có hợp lý không?**\n",
        "- Có, kết quả sinh ra có ngữ pháp đúng và mạch lạc\n",
        "- Văn bản phát triển ý tưởng từ câu mồi một cách tự nhiên\n",
        "- Lưu ý: Kết quả có thể khác nhau mỗi lần chạy do tính ngẫu nhiên của quá trình sinh\n",
        "\n",
        "**2. Tại sao các mô hình Decoder-only như GPT lại phù hợp cho tác vụ này?**\n",
        "- GPT là mô hình **unidirectional** (một chiều), chỉ nhìn các từ đã xuất hiện trước đó\n",
        "- Cơ chế **auto-regressive**: dự đoán từ tiếp theo dựa vào các từ trước, rồi thêm từ đó vào input để dự đoán từ tiếp theo nữa\n",
        "- Kiến trúc này hoàn toàn phù hợp với bản chất của việc sinh văn bản: tạo nội dung tuần tự từ trái sang phải"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bài 3: Tính toán Vector biểu diễn của câu (Sentence Representation)\n",
        "\n",
        "Một trong những ứng dụng mạnh mẽ nhất của BERT là khả năng chuyển đổi một câu thành một vector số có chiều dài cố định, nắm bắt được ngữ nghĩa của câu đó.\n",
        "\n",
        "**Hai cách phổ biến để lấy vector biểu diễn:**\n",
        "1. Lấy vector đầu ra của token `[CLS]`\n",
        "2. Lấy trung bình cộng của các vector đầu ra (Mean Pooling) - thường cho kết quả tốt hơn\n",
        "\n",
        "**Yêu cầu:** Viết code để tính toán vector biểu diễn cho câu `This is a sample sentence.` bằng phương pháp Mean Pooling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff38a20362894ff59d8b4197bee56021",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5610a3ca293b421b8b91be2e6996fb35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb73395329004fd6859bb6eb15020d4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e36a5718b9164fdda72f7b3143ef213d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb84f538b4c7435398f153dcaf0cf4d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['[CLS]', 'this', 'is', 'a', 'sample', 'sentence', '.', '[SEP]']\n",
            "Input IDs: tensor([[ 101, 2023, 2003, 1037, 7099, 6251, 1012,  102]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# 1. Chọn một mô hình BERT\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. Câu đầu vào\n",
        "sentences = [\"This is a sample sentence.\"]\n",
        "\n",
        "# 3. Tokenize câu\n",
        "# padding=True: đệm các câu ngắn hơn để có cùng độ dài\n",
        "# truncation=True: cắt các câu dài hơn\n",
        "# return_tensors='pt': trả về kết quả dưới dạng PyTorch tensors\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))\n",
        "print(\"Input IDs:\", inputs['input_ids'])\n",
        "print(\"Attention Mask:\", inputs['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape của last_hidden_state: torch.Size([1, 8, 768])\n"
          ]
        }
      ],
      "source": [
        "# 4. Đưa qua mô hình để lấy hidden states\n",
        "# torch.no_grad() để không tính toán gradient, tiết kiệm bộ nhớ\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# outputs.last_hidden_state chứa vector đầu ra của tất cả các token\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "# shape: (batch_size, sequence_length, hidden_size)\n",
        "print(f\"Shape của last_hidden_state: {last_hidden_state.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector biểu diễn của câu (10 phần tử đầu):\n",
            "tensor([-6.3875e-02, -4.2837e-01, -6.6779e-02, -3.8430e-01, -6.5785e-02,\n",
            "        -2.1826e-01,  4.7636e-01,  4.8659e-01,  3.9354e-05, -7.4273e-02])\n",
            "\n",
            "Kích thước của vector: torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "# 5. Thực hiện Mean Pooling\n",
        "# Để tính trung bình chính xác, chúng ta cần bỏ qua các token đệm (padding tokens)\n",
        "attention_mask = inputs['attention_mask']\n",
        "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
        "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "sentence_embedding = sum_embeddings / sum_mask\n",
        "\n",
        "# 6. In kết quả\n",
        "print(\"Vector biểu diễn của câu (10 phần tử đầu):\")\n",
        "print(sentence_embedding[0][:10])\n",
        "print(f\"\\nKích thước của vector: {sentence_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phân tích Bài 3:\n",
        "\n",
        "**1. Kích thước (chiều) của vector biểu diễn là bao nhiêu? Con số này tương ứng với tham số nào của mô hình BERT?**\n",
        "- Kích thước vector là **768 chiều**\n",
        "- Con số này tương ứng với tham số `hidden_size` của mô hình `bert-base-uncased`\n",
        "- Các biến thể khác: `bert-large` có hidden_size = 1024\n",
        "\n",
        "**2. Tại sao chúng ta cần sử dụng `attention_mask` khi thực hiện Mean Pooling?**\n",
        "- Khi xử lý batch, các câu ngắn hơn được thêm **padding tokens** để có cùng độ dài\n",
        "- Padding tokens không mang ý nghĩa ngữ nghĩa\n",
        "- Nếu đưa vào phép tính trung bình, kết quả sẽ bị sai lệch\n",
        "- `attention_mask` giúp \"che\" các padding tokens, chỉ tính trung bình trên các token thực sự"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ứng dụng: So sánh độ tương đồng giữa các câu\n",
        "\n",
        "Sử dụng sentence embeddings để tính cosine similarity giữa các câu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ma trận độ tương đồng:\n",
            "\n",
            "Câu:\n",
            "  [0] I love machine learning.\n",
            "  [1] I enjoy studying artificial intelligence.\n",
            "  [2] The weather is nice today.\n",
            "  [3] It is sunny outside.\n",
            "\n",
            "Similarity Matrix:\n",
            "[[1.    0.863 0.589 0.619]\n",
            " [0.863 1.    0.607 0.634]\n",
            " [0.589 0.607 1.    0.839]\n",
            " [0.619 0.634 0.839 1.   ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def get_sentence_embedding(sentence, tokenizer, model):\n",
        "    \"\"\"Tính sentence embedding bằng Mean Pooling\"\"\"\n",
        "    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    attention_mask = inputs['attention_mask']\n",
        "    mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "    sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
        "    return (sum_embeddings / sum_mask).numpy()\n",
        "\n",
        "# Các câu để so sánh\n",
        "sentences = [\n",
        "    \"I love machine learning.\",\n",
        "    \"I enjoy studying artificial intelligence.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"It is sunny outside.\"\n",
        "]\n",
        "\n",
        "# Tính embeddings\n",
        "embeddings = np.vstack([get_sentence_embedding(s, tokenizer, model) for s in sentences])\n",
        "\n",
        "# Tính cosine similarity\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"Ma trận độ tương đồng:\")\n",
        "print(\"\\nCâu:\")\n",
        "for i, s in enumerate(sentences):\n",
        "    print(f\"  [{i}] {s}\")\n",
        "\n",
        "print(\"\\nSimilarity Matrix:\")\n",
        "print(np.round(similarity_matrix, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phân tích kết quả:\n",
        "- Câu [0] và [1] có độ tương đồng cao vì cùng chủ đề về AI/ML\n",
        "- Câu [2] và [3] có độ tương đồng cao vì cùng chủ đề về thời tiết\n",
        "- Các cặp câu khác chủ đề có độ tương đồng thấp hơn"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
